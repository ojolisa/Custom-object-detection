{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:01:10.996111Z",
     "iopub.status.busy": "2025-05-16T09:01:10.995905Z",
     "iopub.status.idle": "2025-05-16T09:01:19.447314Z",
     "shell.execute_reply": "2025-05-16T09:01:19.446712Z",
     "shell.execute_reply.started": "2025-05-16T09:01:10.996084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset, ConcatDataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from collections import defaultdict\n",
    "from torchvision.ops import nms\n",
    "from skmultilearn.model_selection import iterative_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:01:19.448379Z",
     "iopub.status.busy": "2025-05-16T09:01:19.448077Z",
     "iopub.status.idle": "2025-05-16T09:01:19.453455Z",
     "shell.execute_reply": "2025-05-16T09:01:19.452748Z",
     "shell.execute_reply.started": "2025-05-16T09:01:19.448361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Could give error if the dataset server is not reachable.\n",
    "train_dataset = VOCDetection(root='./train', year='2007', image_set='trainval', download=True, transform=transform)\n",
    "val_dataset = VOCDetection(root='./test', year='2007', image_set='test', download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:01:19.455456Z",
     "iopub.status.busy": "2025-05-16T09:01:19.455106Z",
     "iopub.status.idle": "2025-05-16T09:01:19.478253Z",
     "shell.execute_reply": "2025-05-16T09:01:19.477530Z",
     "shell.execute_reply.started": "2025-05-16T09:01:19.455425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "               'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\n",
    "               'horse', 'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "CLASS_TO_IDX = {cls: i for i, cls in enumerate(CLASS_NAMES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:01:19.479127Z",
     "iopub.status.busy": "2025-05-16T09:01:19.478932Z",
     "iopub.status.idle": "2025-05-16T09:01:19.493736Z",
     "shell.execute_reply": "2025-05-16T09:01:19.493067Z",
     "shell.execute_reply.started": "2025-05-16T09:01:19.479113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_set='trainval', year='2007', transform=None, grid_size=7, num_boxes=2):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.grid_size = grid_size\n",
    "        self.num_boxes = num_boxes\n",
    "        self.image_dir = os.path.join(root_dir, 'JPEGImages')\n",
    "        self.annotation_dir = os.path.join(root_dir, 'Annotations')\n",
    "        list_path = os.path.join(root_dir, 'ImageSets', 'Main', f'{image_set}.txt')\n",
    "        with open(list_path) as f:\n",
    "            self.image_ids = [line.strip() for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{image_id}.jpg\")\n",
    "        ann_path = os.path.join(self.annotation_dir, f\"{image_id}.xml\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        original_size = image.size  # (width, height)\n",
    "        boxes, labels = self.parse_voc_xml(ann_path, original_size)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Scale boxes to 448x448 image size\n",
    "        scaled_boxes = []\n",
    "        for box in boxes:\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            xmin = xmin / original_size[0] * 448\n",
    "            xmax = xmax / original_size[0] * 448\n",
    "            ymin = ymin / original_size[1] * 448\n",
    "            ymax = ymax / original_size[1] * 448\n",
    "            scaled_boxes.append([xmin, ymin, xmax, ymax])\n",
    "        \n",
    "        target = self.encode_boxes(scaled_boxes, labels)\n",
    "        \n",
    "        # Also return original boxes and labels for mAP calculation\n",
    "        original_boxes = torch.tensor(scaled_boxes, dtype=torch.float32)\n",
    "        original_labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        return image, target, original_boxes, original_labels, image_id\n",
    "\n",
    "    def parse_voc_xml(self, xml_path, image_size):\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        boxes, labels = [], []\n",
    "\n",
    "        for obj in root.iter('object'):\n",
    "            label = obj.find('name').text\n",
    "            bbox = obj.find('bndbox')\n",
    "            xmin = int(float(bbox.find('xmin').text))\n",
    "            ymin = int(float(bbox.find('ymin').text))\n",
    "            xmax = int(float(bbox.find('xmax').text))\n",
    "            ymax = int(float(bbox.find('ymax').text))\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(CLASS_TO_IDX[label])\n",
    "\n",
    "        return boxes, labels\n",
    "\n",
    "    def encode_boxes(self, boxes, labels):\n",
    "        target = torch.zeros((self.grid_size, self.grid_size, self.num_boxes * 5 + len(CLASS_NAMES)))\n",
    "        img_w, img_h = 448, 448\n",
    "    \n",
    "        for box, label in zip(boxes, labels):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            x_center = (xmin + xmax) / 2.0 / img_w\n",
    "            y_center = (ymin + ymax) / 2.0 / img_h\n",
    "            box_w = (xmax - xmin) / img_w\n",
    "            box_h = (ymax - ymin) / img_h\n",
    "    \n",
    "            grid_x = int(x_center * self.grid_size)\n",
    "            grid_y = int(y_center * self.grid_size)\n",
    "    \n",
    "            if grid_x >= self.grid_size or grid_y >= self.grid_size:\n",
    "                continue\n",
    "    \n",
    "            cell_x = x_center * self.grid_size - grid_x\n",
    "            cell_y = y_center * self.grid_size - grid_y\n",
    "    \n",
    "            if target[grid_y, grid_x, 4].item() == 0: \n",
    "                target[grid_y, grid_x, 0:5] = torch.tensor([cell_x, cell_y, box_w, box_h, 1])\n",
    "                target[grid_y, grid_x, self.num_boxes * 5 + label] = 1\n",
    "\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:01:19.514749Z",
     "iopub.status.busy": "2025-05-16T09:01:19.514536Z",
     "iopub.status.idle": "2025-05-16T09:01:19.595304Z",
     "shell.execute_reply": "2025-05-16T09:01:19.594719Z",
     "shell.execute_reply.started": "2025-05-16T09:01:19.514700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainval_dataset = VOCDataset(\n",
    "    root_dir='/kaggle/input/pascal-voc-2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007',\n",
    "    image_set='trainval',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = VOCDataset(\n",
    "    root_dir='/kaggle/input/pascal-voc-2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007',\n",
    "    image_set='test',\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:01:19.596180Z",
     "iopub.status.busy": "2025-05-16T09:01:19.596000Z",
     "iopub.status.idle": "2025-05-16T09:05:27.689504Z",
     "shell.execute_reply": "2025-05-16T09:05:27.688640Z",
     "shell.execute_reply.started": "2025-05-16T09:01:19.596165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_image_labels(dataset):\n",
    "    image_labels = []\n",
    "    for i in range(len(dataset)):\n",
    "        try:\n",
    "            _, _, _, labels, _ = dataset[i] \n",
    "            unique_labels = set(labels)  \n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {i}: {e}\")\n",
    "            unique_labels = set()\n",
    "        image_labels.append(unique_labels)\n",
    "    return image_labels\n",
    "\n",
    "def labels_to_multihot(labels_list, num_classes=20):\n",
    "    multihot = np.zeros((len(labels_list), num_classes), dtype=int)\n",
    "    for idx, labels in enumerate(labels_list):\n",
    "        for label in labels:\n",
    "            multihot[idx, label] = 1\n",
    "    return multihot\n",
    "\n",
    "full_dataset = ConcatDataset([trainval_dataset, test_dataset])\n",
    "\n",
    "image_labels = get_image_labels(full_dataset)\n",
    "X = np.arange(len(full_dataset)).reshape(-1, 1)\n",
    "Y = labels_to_multihot(image_labels, num_classes=len(CLASS_NAMES))\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 80% train, 20% temp\n",
    "X_train, Y_train, X_temp, Y_temp = iterative_train_test_split(X, Y, test_size=0.2)\n",
    "# 10% val, 10% test\n",
    "X_val, Y_val, X_test, Y_test = iterative_train_test_split(X_temp, Y_temp, test_size=0.5)\n",
    "\n",
    "train_indices = X_train.flatten().tolist()\n",
    "val_indices = X_val.flatten().tolist()\n",
    "test_indices = X_test.flatten().tolist()\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "def custom_collate(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    encoded_targets = torch.stack([item[1] for item in batch]) # Stack encoded targets for loss calculation\n",
    "    original_boxes = [item[2] for item in batch]      # Keep as list of tensors for mAP\n",
    "    original_labels = [item[3] for item in batch]     # Keep as list of tensors for mAP\n",
    "    image_ids = [item[4] for item in batch]           # Keep as list of strings\n",
    "    return images, encoded_targets, original_boxes, original_labels, image_ids\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:05:27.714237Z",
     "iopub.status.busy": "2025-05-16T09:05:27.713985Z",
     "iopub.status.idle": "2025-05-16T09:05:27.729209Z",
     "shell.execute_reply": "2025-05-16T09:05:27.728681Z",
     "shell.execute_reply.started": "2025-05-16T09:05:27.714216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class YOLOHead(nn.Module):\n",
    "    def __init__(self, in_channels, grid_size=7, num_boxes=2, num_classes=20):\n",
    "        super(YOLOHead, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        self.output_dim = grid_size * grid_size * (num_boxes * 5 + num_classes)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((self.grid_size, self.grid_size))\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * grid_size * grid_size, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(4096, self.output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.fc(x)\n",
    "        return x.view(-1, self.grid_size, self.grid_size, self.num_boxes * 5 + self.num_classes)\n",
    "\n",
    "class YOLOResNet(nn.Module):\n",
    "    def __init__(self, grid_size=7, num_boxes=2, num_classes=20):\n",
    "        super(YOLOResNet, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])  # remove avgpool & fc\n",
    "        self.yolo_head = YOLOHead(in_channels=2048, grid_size=grid_size, num_boxes=num_boxes, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.yolo_head(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:27:03.737839Z",
     "iopub.status.busy": "2025-05-16T09:27:03.737285Z",
     "iopub.status.idle": "2025-05-16T09:27:07.412386Z",
     "shell.execute_reply": "2025-05-16T09:27:07.411616Z",
     "shell.execute_reply.started": "2025-05-16T09:27:03.737816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YOLOResNet().to(device)\n",
    "model.load_state_dict(torch.load('YOUR_MODEL_PATH.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yolo_output_to_boxes(output, grid_size=7, num_boxes=2, num_classes=20, conf_thresh=0.25):\n",
    "\n",
    "    batch_size = output.shape[0]\n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    all_class_idxs = []\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        boxes = []\n",
    "        scores = []\n",
    "        class_idxs = []\n",
    "        \n",
    "        pred = output[b]  # (S, S, B*5+C)\n",
    "        \n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                cell_pred = pred[i, j]\n",
    "                \n",
    "                # Process both bounding boxes in the cell\n",
    "                for box_idx in range(num_boxes):\n",
    "                    box_start = box_idx * 5\n",
    "                    confidence = cell_pred[box_start + 4].item()\n",
    "                    \n",
    "                    # Only consider boxes with confidence above threshold\n",
    "                    if confidence < conf_thresh:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get box coordinates\n",
    "                    x_cell, y_cell = cell_pred[box_start:box_start+2]\n",
    "                    w, h = cell_pred[box_start+2:box_start+4]\n",
    "                    \n",
    "                    # Convert to absolute coordinates (0-1 scale)\n",
    "                    x_center = (j + x_cell) / grid_size\n",
    "                    y_center = (i + y_cell) / grid_size\n",
    "                    w = w.clamp(0, 1)  # ensure width is positive and within bounds\n",
    "                    h = h.clamp(0, 1)  # ensure height is positive and within bounds\n",
    "                    \n",
    "                    # Convert to corners format [x1, y1, x2, y2] (still 0-1 scale)\n",
    "                    x1 = (x_center - w/2).clamp(0, 1)\n",
    "                    y1 = (y_center - h/2).clamp(0, 1)\n",
    "                    x2 = (x_center + w/2).clamp(0, 1)\n",
    "                    y2 = (y_center + h/2).clamp(0, 1)\n",
    "                    \n",
    "                    # Scaling to image size (448x448)\n",
    "                    box = [x1.item() * 448, y1.item() * 448, x2.item() * 448, y2.item() * 448]\n",
    "                    \n",
    "                    # Find class with highest probability\n",
    "                    class_scores = cell_pred[num_boxes*5:]\n",
    "                    class_idx = torch.argmax(class_scores).item()\n",
    "                    class_score = class_scores[class_idx].item()\n",
    "                    \n",
    "                    # Final score is class_score * confidence\n",
    "                    score = class_score * confidence\n",
    "                    \n",
    "                    boxes.append(box)\n",
    "                    scores.append(score)\n",
    "                    class_idxs.append(class_idx)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        if boxes:  # Check if any boxes were detected\n",
    "            all_boxes.append(torch.tensor(boxes))\n",
    "            all_scores.append(torch.tensor(scores))\n",
    "            all_class_idxs.append(torch.tensor(class_idxs))\n",
    "        else:  # No detections for this image\n",
    "            all_boxes.append(torch.zeros((0, 4)))\n",
    "            all_scores.append(torch.zeros(0))\n",
    "            all_class_idxs.append(torch.zeros(0, dtype=torch.int64))\n",
    "    \n",
    "    return all_boxes, all_scores, all_class_idxs\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "\n",
    "    # Get coordinates of intersection rectangle\n",
    "    x1 = torch.max(box1[0], box2[0])\n",
    "    y1 = torch.max(box1[1], box2[1])\n",
    "    x2 = torch.min(box1[2], box2[2])\n",
    "    y2 = torch.min(box1[3], box2[3])\n",
    "    \n",
    "    # Check if there is intersection\n",
    "    if x2 < x1 or y2 < y1:\n",
    "        return 0.0\n",
    "    \n",
    "    # Area of intersection\n",
    "    intersection_area = (x2 - x1) * (y2 - y1)\n",
    "    \n",
    "    # Area of both boxes\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "    \n",
    "    return iou\n",
    "\n",
    "def calculate_map(\n",
    "    all_pred_boxes_batch,  # List of tensors (one per image)\n",
    "    all_pred_scores_batch, # List of tensors\n",
    "    all_pred_classes_batch,# List of tensors\n",
    "    all_gt_boxes_batch,    # List of tensors\n",
    "    all_gt_classes_batch,  # List of tensors\n",
    "    num_classes=20,\n",
    "    iou_threshold=0.5\n",
    "):\n",
    "    \n",
    "    average_precisions = {}\n",
    "    epsilon = 1e-8 # For numerical stability\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        # 1. Collect all detections and ground truths for the current class\n",
    "        detections_for_class = []  # List of tuples: (score, image_idx, pred_box)\n",
    "        ground_truths_for_class_by_image = defaultdict(list) # {image_idx: [{'box': gt_box, 'used': False}, ...]}\n",
    "        total_gt_for_class = 0\n",
    "\n",
    "        for i in range(len(all_pred_boxes_batch)): # Iterate over images\n",
    "            # Ground truths for this image and class\n",
    "            gt_boxes_img = all_gt_boxes_batch[i]\n",
    "            gt_classes_img = all_gt_classes_batch[i]\n",
    "\n",
    "            class_gt_indices = (gt_classes_img == class_idx).nonzero(as_tuple=True)[0]\n",
    "            for gt_idx in class_gt_indices:\n",
    "                ground_truths_for_class_by_image[i].append({\n",
    "                    'box': gt_boxes_img[gt_idx],\n",
    "                    'used': False\n",
    "                })\n",
    "                total_gt_for_class += 1\n",
    "\n",
    "            # Predictions for this image and class\n",
    "            pred_boxes_img = all_pred_boxes_batch[i]\n",
    "            pred_scores_img = all_pred_scores_batch[i]\n",
    "            pred_classes_img = all_pred_classes_batch[i]\n",
    "\n",
    "            class_pred_indices = (pred_classes_img == class_idx).nonzero(as_tuple=True)[0]\n",
    "            for pred_idx in class_pred_indices:\n",
    "                detections_for_class.append({\n",
    "                    'score': pred_scores_img[pred_idx].item(), # Store as float\n",
    "                    'image_idx': i,\n",
    "                    'box': pred_boxes_img[pred_idx]\n",
    "                })\n",
    "\n",
    "        # If no ground truths for this class, AP is 0 (or 1 if no predictions either, common is 0)\n",
    "        if total_gt_for_class == 0:\n",
    "            average_precisions[class_idx] = 0.0\n",
    "            continue\n",
    "\n",
    "        # If no detections for this class, AP is 0\n",
    "        if not detections_for_class:\n",
    "            average_precisions[class_idx] = 0.0\n",
    "            continue\n",
    "\n",
    "        # 2. Sort detections by confidence score (descending)\n",
    "        detections_for_class.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        # 3. Calculate TP and FP for sorted detections\n",
    "        num_detections = len(detections_for_class)\n",
    "        tp_arr = torch.zeros(num_detections)\n",
    "        fp_arr = torch.zeros(num_detections)\n",
    "\n",
    "        for det_idx, det in enumerate(detections_for_class):\n",
    "            img_idx_of_det = det['image_idx']\n",
    "            pred_box = det['box']\n",
    "\n",
    "            # Get GTs for the image of the current detection\n",
    "            gt_objects_in_image = ground_truths_for_class_by_image[img_idx_of_det]\n",
    "\n",
    "            best_iou = -1.0\n",
    "            best_gt_match_idx = -1\n",
    "\n",
    "            for gt_obj_idx, gt_obj in enumerate(gt_objects_in_image):\n",
    "                iou = calculate_iou(pred_box, gt_obj['box'])\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_match_idx = gt_obj_idx\n",
    "\n",
    "            if best_iou >= iou_threshold:\n",
    "                # Check if this GT has already been used for a higher-score detection\n",
    "                if not gt_objects_in_image[best_gt_match_idx]['used']:\n",
    "                    tp_arr[det_idx] = 1\n",
    "                    gt_objects_in_image[best_gt_match_idx]['used'] = True\n",
    "                else: # GT already used by a higher score detection\n",
    "                    fp_arr[det_idx] = 1\n",
    "            else: # No GT match or IoU too low\n",
    "                fp_arr[det_idx] = 1\n",
    "\n",
    "        # 4. Calculate Precision and Recall\n",
    "        tp_cumsum = torch.cumsum(tp_arr, dim=0)\n",
    "        fp_cumsum = torch.cumsum(fp_arr, dim=0)\n",
    "\n",
    "        recalls = tp_cumsum / (total_gt_for_class + epsilon)\n",
    "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + epsilon)\n",
    "\n",
    "        precisions = torch.cat((torch.tensor([1.0]), precisions)) # P at R=0 (or P for first point if R > 0)\n",
    "        recalls = torch.cat((torch.tensor([0.0]), recalls))     # R=0\n",
    "\n",
    "        # Ensure precision is monotonically decreasing (PASCAL VOC 2010+ method)\n",
    "        for i in range(len(precisions) - 2, -1, -1): # Iterate backwards\n",
    "            precisions[i] = torch.max(precisions[i], precisions[i+1])\n",
    "\n",
    "        # Find points where recall changes\n",
    "        recall_changes_indices = torch.where(recalls[1:] != recalls[:-1])[0]\n",
    "\n",
    "        # AP is sum of (recall_i+1 - recall_i) * precision_i+1\n",
    "        ap = torch.sum((recalls[recall_changes_indices + 1] - recalls[recall_changes_indices]) * precisions[recall_changes_indices + 1])\n",
    "\n",
    "        average_precisions[class_idx] = ap.item()\n",
    "\n",
    "    # Calculate mAP\n",
    "    valid_aps = [ap for ap in average_precisions.values() if not torch.isnan(torch.tensor(ap))] # Filter out NaNs if any\n",
    "    if not valid_aps:\n",
    "         mean_ap = 0.0\n",
    "    else:\n",
    "        mean_ap = sum(valid_aps) / len(valid_aps) if valid_aps else 0.0\n",
    "\n",
    "    return mean_ap, average_precisions\n",
    "\n",
    "def calculate_precision_recall(pred_boxes, pred_scores, pred_classes, gt_boxes, gt_classes, num_classes=20, iou_threshold=0.5):\n",
    "\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    \n",
    "    # Process each image\n",
    "    for i in range(len(pred_boxes)):\n",
    "        # Get predictions and ground truth for this image\n",
    "        boxes = pred_boxes[i]\n",
    "        scores = pred_scores[i]\n",
    "        classes = pred_classes[i]\n",
    "        \n",
    "        gt_b = gt_boxes[i]\n",
    "        gt_c = gt_classes[i]\n",
    "        \n",
    "        # Initialize array to keep track of which ground truth boxes have been detected\n",
    "        gt_detected = torch.zeros(len(gt_b))\n",
    "        \n",
    "        # Count true positives and false positives\n",
    "        for j in range(len(boxes)):\n",
    "            # Calculate IoU with all ground truth boxes\n",
    "            max_iou = 0\n",
    "            max_idx = -1\n",
    "            \n",
    "            for k in range(len(gt_b)):\n",
    "                # Only consider ground truth boxes of the same class\n",
    "                if gt_c[k] != classes[j]:\n",
    "                    continue\n",
    "                \n",
    "                iou = calculate_iou(boxes[j], gt_b[k])\n",
    "                if iou > max_iou:\n",
    "                    max_iou = iou\n",
    "                    max_idx = k\n",
    "            \n",
    "            # Check if prediction matches a ground truth box\n",
    "            if max_iou >= iou_threshold and gt_detected[max_idx] == 0:\n",
    "                total_tp += 1\n",
    "                gt_detected[max_idx] = 1  # Mark as detected\n",
    "            else:\n",
    "                total_fp += 1\n",
    "        \n",
    "        # Count false negatives (ground truth boxes that weren't detected)\n",
    "        total_fn += (1 - gt_detected).sum().item()\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = total_tp / (total_tp + total_fp + 1e-8)\n",
    "    recall = total_tp / (total_tp + total_fn + 1e-8)\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_pred_boxes = []\n",
    "    all_pred_scores = []\n",
    "    all_pred_classes = []\n",
    "    all_gt_boxes = []\n",
    "    all_gt_classes = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _, gt_boxes, gt_classes, _ in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Convert outputs to boxes\n",
    "            pred_boxes, pred_scores, pred_classes = convert_yolo_output_to_boxes(outputs)\n",
    "            \n",
    "            # Append to lists\n",
    "            all_pred_boxes.extend(pred_boxes)\n",
    "            all_pred_scores.extend(pred_scores)\n",
    "            all_pred_classes.extend(pred_classes)\n",
    "            all_gt_boxes.extend(gt_boxes)\n",
    "            all_gt_classes.extend(gt_classes)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    map_score, ap_per_class = calculate_map(all_pred_boxes, all_pred_scores, all_pred_classes, \n",
    "                                           all_gt_boxes, all_gt_classes)\n",
    "    precision, recall = calculate_precision_recall(all_pred_boxes, all_pred_scores, all_pred_classes,\n",
    "                                                 all_gt_boxes, all_gt_classes)\n",
    "    \n",
    "    return map_score, precision, recall, ap_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:22:46.389301Z",
     "iopub.status.busy": "2025-05-16T09:22:46.388995Z",
     "iopub.status.idle": "2025-05-16T09:25:51.796941Z",
     "shell.execute_reply": "2025-05-16T09:25:51.796196Z",
     "shell.execute_reply.started": "2025-05-16T09:22:46.389275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Final Evaluation on Data Set ===\")\n",
    "\n",
    "import itertools\n",
    "combined_loader = itertools.chain(train_loader, test_loader, val_loader)\n",
    "\n",
    "test_map, test_precision, test_recall, test_ap_per_class = evaluate_model(model, train_loader, device)\n",
    "\n",
    "print(f\"Test mAP@0.5: {test_map:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Class-wise AP scores\n",
    "print(\"\\nClass-wise AP scores:\")\n",
    "for cls_idx, ap in test_ap_per_class.items():\n",
    "    print(f\"{CLASS_NAMES[cls_idx]}: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:27:13.205433Z",
     "iopub.status.busy": "2025-05-16T09:27:13.205098Z",
     "iopub.status.idle": "2025-05-16T09:27:13.986573Z",
     "shell.execute_reply": "2025-05-16T09:27:13.985667Z",
     "shell.execute_reply.started": "2025-05-16T09:27:13.205411Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize_prediction_with_metrics(model, image, target, original_boxes, original_labels, iou_threshold=0.5, score_threshold=0.3):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get model prediction\n",
    "        input_tensor = image.unsqueeze(0).to(device)\n",
    "        output = model(input_tensor).squeeze(0)\n",
    "        \n",
    "        # Convert output to boxes\n",
    "        pred_boxes, pred_scores, pred_classes = convert_yolo_output_to_boxes(output.unsqueeze(0))\n",
    "        pred_boxes = pred_boxes[0]\n",
    "        pred_scores = pred_scores[0]\n",
    "        pred_classes = pred_classes[0]\n",
    "        \n",
    "        # Filter by score threshold\n",
    "        keep = pred_scores > score_threshold\n",
    "        pred_boxes = pred_boxes[keep]\n",
    "        pred_scores = pred_scores[keep]\n",
    "        pred_classes = pred_classes[keep]\n",
    "\n",
    "        # Apply NMS\n",
    "        if len(pred_boxes) > 0:\n",
    "            keep_idx = nms(pred_boxes, pred_scores, iou_threshold)\n",
    "            pred_boxes = pred_boxes[keep_idx]\n",
    "            pred_scores = pred_scores[keep_idx]\n",
    "            pred_classes = pred_classes[keep_idx]\n",
    "\n",
    "        # Visualization code (same as before)\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "        h, w = image_np.shape[:2]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "        \n",
    "        # Ground truth\n",
    "        ax1.imshow(image_np)\n",
    "        ax1.set_title(\"Ground Truth\")\n",
    "        \n",
    "        for box, label in zip(original_boxes, original_labels):\n",
    "            x1, y1, x2, y2 = box\n",
    "            class_idx = label.item()\n",
    "            class_name = CLASS_NAMES[class_idx]\n",
    "\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='g', facecolor='none')\n",
    "            ax1.add_patch(rect)\n",
    "            ax1.text(x1, y1-5, class_name, color='white', fontsize=8, backgroundcolor='green')\n",
    "        \n",
    "        # Predictions\n",
    "        ax2.imshow(image_np)\n",
    "        ax2.set_title(\"Predictions\")\n",
    "        \n",
    "        for box, score, class_idx in zip(pred_boxes, pred_scores, pred_classes):\n",
    "            x1, y1, x2, y2 = box\n",
    "            class_name = CLASS_NAMES[class_idx.item()]\n",
    "            \n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax2.add_patch(rect)\n",
    "            ax2.text(x1, y1-5, f'{class_name} ({score:.2f})', color='white', fontsize=8, backgroundcolor='red')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Visualize a sample from test set\n",
    "sample_idx = 255 # Change this to view different samples\n",
    "img, target, orig_boxes, orig_labels, _ = test_dataset[sample_idx]\n",
    "visualize_prediction_with_metrics(model, img, target, orig_boxes, orig_labels)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 18276,
     "sourceId": 23902,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 345241,
     "modelInstanceId": 324447,
     "sourceId": 394622,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
