{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-15T12:47:23.308409Z",
     "iopub.status.busy": "2025-05-15T12:47:23.308155Z",
     "iopub.status.idle": "2025-05-15T12:47:31.810744Z",
     "shell.execute_reply": "2025-05-15T12:47:31.809958Z",
     "shell.execute_reply.started": "2025-05-15T12:47:23.308389Z"
    },
    "id": "w2VJIZxQTECZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset, ConcatDataset\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.ops import nms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-15T12:47:31.812248Z",
     "iopub.status.busy": "2025-05-15T12:47:31.811951Z",
     "iopub.status.idle": "2025-05-15T12:48:14.662966Z",
     "shell.execute_reply": "2025-05-15T12:48:14.662375Z",
     "shell.execute_reply.started": "2025-05-15T12:47:31.812230Z"
    },
    "id": "s0hM-3xLTECb",
    "outputId": "c3003c37-5a5a-4915-aa38-d123c4bf2aff",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = VOCDetection(root='./train', year='2007', image_set='trainval', download=True, transform=transform)\n",
    "val_dataset = VOCDetection(root='./test', year='2007', image_set='test', download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T12:48:14.663945Z",
     "iopub.status.busy": "2025-05-15T12:48:14.663732Z",
     "iopub.status.idle": "2025-05-15T12:48:14.668099Z",
     "shell.execute_reply": "2025-05-15T12:48:14.667563Z",
     "shell.execute_reply.started": "2025-05-15T12:48:14.663928Z"
    },
    "id": "4-FxxXR9TECb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "               'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\n",
    "               'horse', 'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "CLASS_TO_IDX = {cls: i for i, cls in enumerate(CLASS_NAMES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T12:48:14.670124Z",
     "iopub.status.busy": "2025-05-15T12:48:14.669933Z",
     "iopub.status.idle": "2025-05-15T12:48:14.686532Z",
     "shell.execute_reply": "2025-05-15T12:48:14.685848Z",
     "shell.execute_reply.started": "2025-05-15T12:48:14.670109Z"
    },
    "id": "LCEyMjxBTECc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_set='trainval', year='2007', transform=None, grid_size=7, num_boxes=2):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.grid_size = grid_size\n",
    "        self.num_boxes = num_boxes\n",
    "        self.image_dir = os.path.join(root_dir, 'JPEGImages')\n",
    "        self.annotation_dir = os.path.join(root_dir, 'Annotations')\n",
    "        list_path = os.path.join(root_dir, 'ImageSets', 'Main', f'{image_set}.txt')\n",
    "        with open(list_path) as f:\n",
    "            self.image_ids = [line.strip() for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{image_id}.jpg\")\n",
    "        ann_path = os.path.join(self.annotation_dir, f\"{image_id}.xml\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        original_size = image.size  # (width, height)\n",
    "        boxes, labels = self.parse_voc_xml(ann_path, original_size)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Scale boxes to 448x448 image size\n",
    "        scaled_boxes = []\n",
    "        for box in boxes:\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            xmin = xmin / original_size[0] * 448\n",
    "            xmax = xmax / original_size[0] * 448\n",
    "            ymin = ymin / original_size[1] * 448\n",
    "            ymax = ymax / original_size[1] * 448\n",
    "            scaled_boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        target = self.encode_boxes(scaled_boxes, labels)\n",
    "\n",
    "        # Also return original boxes and labels for mAP calculation\n",
    "        original_boxes = torch.tensor(scaled_boxes, dtype=torch.float32)\n",
    "        original_labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        return image, target, original_boxes, original_labels, image_id\n",
    "\n",
    "    def parse_voc_xml(self, xml_path, image_size):\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        boxes, labels = [], []\n",
    "\n",
    "        for obj in root.iter('object'):\n",
    "            label = obj.find('name').text\n",
    "            bbox = obj.find('bndbox')\n",
    "            xmin = int(float(bbox.find('xmin').text))\n",
    "            ymin = int(float(bbox.find('ymin').text))\n",
    "            xmax = int(float(bbox.find('xmax').text))\n",
    "            ymax = int(float(bbox.find('ymax').text))\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(CLASS_TO_IDX[label])\n",
    "\n",
    "        return boxes, labels\n",
    "\n",
    "    def encode_boxes(self, boxes, labels):\n",
    "        target = torch.zeros((self.grid_size, self.grid_size, self.num_boxes * 5 + len(CLASS_NAMES)))\n",
    "        img_w, img_h = 448, 448\n",
    "\n",
    "        for box, label in zip(boxes, labels):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            x_center = (xmin + xmax) / 2.0 / img_w\n",
    "            y_center = (ymin + ymax) / 2.0 / img_h\n",
    "            box_w = (xmax - xmin) / img_w\n",
    "            box_h = (ymax - ymin) / img_h\n",
    "\n",
    "            grid_x = int(x_center * self.grid_size)\n",
    "            grid_y = int(y_center * self.grid_size)\n",
    "\n",
    "            if grid_x >= self.grid_size or grid_y >= self.grid_size:\n",
    "                continue\n",
    "\n",
    "            # Convert to cell-relative coordinates\n",
    "            cell_x = x_center * self.grid_size - grid_x\n",
    "            cell_y = y_center * self.grid_size - grid_y\n",
    "\n",
    "            for b in range(self.num_boxes):\n",
    "                target[grid_y, grid_x, b*5:(b+1)*5] = torch.tensor([cell_x, cell_y, box_w, box_h, 1])\n",
    "\n",
    "            target[grid_y, grid_x, self.num_boxes * 5 + label] = 1\n",
    "\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T12:48:14.707678Z",
     "iopub.status.busy": "2025-05-15T12:48:14.707509Z",
     "iopub.status.idle": "2025-05-15T12:48:14.724753Z",
     "shell.execute_reply": "2025-05-15T12:48:14.723897Z",
     "shell.execute_reply.started": "2025-05-15T12:48:14.707664Z"
    },
    "id": "O5n01kZ2TECc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainval_dataset = VOCDataset(\n",
    "    root_dir='train/VOCdevkit/VOC2007',\n",
    "    image_set='trainval',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_root = 'test/VOCdevkit/VOC2007'\n",
    "\n",
    "test_dataset = VOCDataset(\n",
    "    root_dir=test_root,\n",
    "    image_set='test',\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T12:48:14.726024Z",
     "iopub.status.busy": "2025-05-15T12:48:14.725496Z",
     "iopub.status.idle": "2025-05-15T12:49:16.480418Z",
     "shell.execute_reply": "2025-05-15T12:49:16.479746Z",
     "shell.execute_reply.started": "2025-05-15T12:48:14.726000Z"
    },
    "id": "ou9gtI5NTECc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_image_labels(dataset):\n",
    "    image_labels = []\n",
    "    for i in range(len(dataset)):\n",
    "        try:\n",
    "            _, _, _, labels, _ = dataset[i]  \n",
    "            unique_labels = set(labels) \n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {i}: {e}\")\n",
    "            unique_labels = set()\n",
    "        image_labels.append(unique_labels)\n",
    "    return image_labels\n",
    "\n",
    "def labels_to_multihot(labels_list, num_classes=20):\n",
    "    multihot = np.zeros((len(labels_list), num_classes), dtype=int)\n",
    "    for idx, labels in enumerate(labels_list):\n",
    "        for label in labels:\n",
    "            multihot[idx, label] = 1\n",
    "    return multihot\n",
    "\n",
    "full_dataset = ConcatDataset([trainval_dataset, test_dataset])\n",
    "\n",
    "image_labels = get_image_labels(full_dataset)\n",
    "X = np.arange(len(full_dataset)).reshape(-1, 1)\n",
    "Y = labels_to_multihot(image_labels, num_classes=len(CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T12:49:16.481403Z",
     "iopub.status.busy": "2025-05-15T12:49:16.481181Z",
     "iopub.status.idle": "2025-05-15T12:49:16.485148Z",
     "shell.execute_reply": "2025-05-15T12:49:16.484454Z",
     "shell.execute_reply.started": "2025-05-15T12:49:16.481385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T12:49:16.486818Z",
     "iopub.status.busy": "2025-05-15T12:49:16.486034Z",
     "iopub.status.idle": "2025-05-15T12:49:16.847296Z",
     "shell.execute_reply": "2025-05-15T12:49:16.846513Z",
     "shell.execute_reply.started": "2025-05-15T12:49:16.486789Z"
    },
    "id": "StvCNoNQTECc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 80% train, 20% temp\n",
    "X_train, Y_train, X_temp, Y_temp = iterative_train_test_split(X, Y, test_size=0.2)\n",
    "# 10% val, 10% test\n",
    "X_val, Y_val, X_test, Y_test = iterative_train_test_split(X_temp, Y_temp, test_size=0.5)\n",
    "\n",
    "train_indices = X_train.flatten().tolist()\n",
    "val_indices = X_val.flatten().tolist()\n",
    "test_indices = X_test.flatten().tolist()\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "def custom_collate(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    encoded_targets = torch.stack([item[1] for item in batch]) \n",
    "    original_boxes = [item[2] for item in batch]      \n",
    "    original_labels = [item[3] for item in batch]     \n",
    "    image_ids = [item[4] for item in batch]          \n",
    "    return images, encoded_targets, original_boxes, original_labels, image_ids\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "execution": {
     "iopub.execute_input": "2025-05-15T12:49:16.850259Z",
     "iopub.status.busy": "2025-05-15T12:49:16.850024Z",
     "iopub.status.idle": "2025-05-15T12:49:17.094803Z",
     "shell.execute_reply": "2025-05-15T12:49:17.094116Z",
     "shell.execute_reply.started": "2025-05-15T12:49:16.850242Z"
    },
    "id": "aATTFpoBTECd",
    "outputId": "e07e606e-e2c0-4791-8f8b-135f6cfbc9e3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def show_image_with_boxes(image, target, grid_size=7, class_names=CLASS_NAMES, conf_thresh=0.5):\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            cell = target[i, j]\n",
    "            for b in range(2):  # num_boxes\n",
    "                conf = cell[b*5 + 4].item()\n",
    "                if conf > conf_thresh:\n",
    "                    x, y, box_w, box_h = cell[b*5:b*5+4]\n",
    "                    x = (j + x.item()) / grid_size * w\n",
    "                    y = (i + y.item()) / grid_size * h\n",
    "                    bw = box_w.item() * w\n",
    "                    bh = box_h.item() * h\n",
    "\n",
    "                    # Draw bounding box\n",
    "                    rect = patches.Rectangle((x - bw/2, y - bh/2), bw, bh, linewidth=2, edgecolor='r', facecolor='none')\n",
    "                    ax.add_patch(rect)\n",
    "\n",
    "                    # Find class\n",
    "                    class_probs = cell[2*5:]  \n",
    "                    class_idx = class_probs.argmax().item()\n",
    "                    class_name = class_names[class_idx]\n",
    "                    score = class_probs[class_idx].item()\n",
    "\n",
    "                    # Label\n",
    "                    ax.text(x - bw/2, y - bh/2 - 5, f'{class_name} ({score:.2f})', color='white',\n",
    "                            fontsize=8, backgroundcolor='red')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "img, tgt, _, _, _ = test_dataset[4]\n",
    "show_image_with_boxes(img, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-15T12:49:17.096117Z",
     "iopub.status.busy": "2025-05-15T12:49:17.095904Z",
     "iopub.status.idle": "2025-05-15T12:49:17.121953Z",
     "shell.execute_reply": "2025-05-15T12:49:17.121356Z",
     "shell.execute_reply.started": "2025-05-15T12:49:17.096101Z"
    },
    "id": "BFuPVzdgTECd",
    "outputId": "caf4eb41-626f-4be1-f2a5-064e99324391",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "print(\"Train Dataset:\")\n",
    "print(\"Number of images:\", len(train_dataset))\n",
    "img, target, _, _, _ = train_dataset[0]\n",
    "print(\"Image shape:\", img.shape)\n",
    "print(\"Target shape\", target.shape)\n",
    "\n",
    "# Validation set\n",
    "print(\"\\nValidation Dataset:\")\n",
    "print(\"Number of images:\", len(val_dataset))\n",
    "img, target, _, _, _ = val_dataset[0]\n",
    "print(\"Image shape:\", img.shape)\n",
    "print(\"Target shape\", target.shape)\n",
    "\n",
    "# Test set\n",
    "print(\"\\nTest Dataset:\")\n",
    "print(\"Number of images:\", len(test_dataset))\n",
    "img, target, _, _, _ = test_dataset[0]\n",
    "print(\"Image shape:\", img.shape)\n",
    "print(\"Target shape\", target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T12:49:17.122842Z",
     "iopub.status.busy": "2025-05-15T12:49:17.122636Z",
     "iopub.status.idle": "2025-05-15T12:49:17.133376Z",
     "shell.execute_reply": "2025-05-15T12:49:17.132759Z",
     "shell.execute_reply.started": "2025-05-15T12:49:17.122826Z"
    },
    "id": "Hg9Uu9KiTECd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class YOLOHead(nn.Module):\n",
    "    def __init__(self, in_channels, grid_size=7, num_boxes=2, num_classes=20):\n",
    "        super(YOLOHead, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        self.output_dim = grid_size * grid_size * (num_boxes * 5 + num_classes)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * grid_size * grid_size, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(4096, self.output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.fc(x)\n",
    "        return x.view(-1, self.grid_size, self.grid_size, self.num_boxes * 5 + self.num_classes)\n",
    "\n",
    "class YOLOResNet(nn.Module):\n",
    "    def __init__(self, grid_size=7, num_boxes=2, num_classes=20):\n",
    "        super(YOLOResNet, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.yolo_head = YOLOHead(in_channels=2048, grid_size=grid_size, num_boxes=num_boxes, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.yolo_head(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T12:49:17.134285Z",
     "iopub.status.busy": "2025-05-15T12:49:17.134079Z",
     "iopub.status.idle": "2025-05-15T12:49:17.156601Z",
     "shell.execute_reply": "2025-05-15T12:49:17.155897Z",
     "shell.execute_reply.started": "2025-05-15T12:49:17.134269Z"
    },
    "id": "539-clXkTECd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=20, lambda_coord=5, lambda_noobj=0.5):\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        self.S = S\n",
    "        self.B = B  \n",
    "        self.C = C  \n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        N = predictions.size(0)\n",
    "        predictions = predictions.view(N, self.S, self.S, self.C + self.B * 5)\n",
    "        target = target.view(N, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        pred_classes = predictions[..., self.B * 5:]                         \n",
    "        pred_boxes = predictions[..., :self.B * 5].view(N, self.S, self.S, self.B, 5)  \n",
    "\n",
    "        target_classes = target[..., self.B * 5:]\n",
    "        target_boxes = target[..., :self.B * 5].view(N, self.S, self.S, self.B, 5)\n",
    "\n",
    "        object_mask = target_boxes[..., 4] > 0  \n",
    "\n",
    "        # ===================== #\n",
    "        #   CLASSIFICATION LOSS\n",
    "        # ===================== #\n",
    "        has_object = (target_boxes[..., 4].sum(dim=-1) > 0)  \n",
    "        class_loss = self.mse(\n",
    "            pred_classes[has_object],\n",
    "            target_classes[has_object]\n",
    "        )\n",
    "\n",
    "        # ===================== #\n",
    "        #   BOX COORDINATE LOSS\n",
    "        # ===================== #\n",
    "        box_pred = pred_boxes[object_mask]  \n",
    "        box_target = target_boxes[object_mask]\n",
    "\n",
    "        coord_loss = self.mse(box_pred[..., 0:2], box_target[..., 0:2])  \n",
    "\n",
    "        pred_wh = torch.sign(box_pred[..., 2:4]) * torch.sqrt(torch.abs(box_pred[..., 2:4]) + 1e-6)\n",
    "        target_wh = torch.sqrt(box_target[..., 2:4])\n",
    "        coord_loss += self.mse(pred_wh, target_wh)\n",
    "\n",
    "        coord_loss *= self.lambda_coord\n",
    "\n",
    "        # ===================== #\n",
    "        #   OBJECT CONFIDENCE LOSS\n",
    "        # ===================== #\n",
    "        object_loss = self.mse(box_pred[..., 4], box_target[..., 4])  # confidence\n",
    "\n",
    "        # ===================== #\n",
    "        #   NO OBJECT LOSS\n",
    "        # ===================== #\n",
    "        noobj_mask = ~object_mask\n",
    "        noobj_pred = pred_boxes[noobj_mask][..., 4]\n",
    "        noobj_target = target_boxes[noobj_mask][..., 4]\n",
    "        noobj_loss = self.mse(noobj_pred, noobj_target)\n",
    "        noobj_loss *= self.lambda_noobj\n",
    "\n",
    "        # ===================== #\n",
    "        #   TOTAL LOSS\n",
    "        # ===================== #\n",
    "        total_loss = class_loss + coord_loss + object_loss + noobj_loss\n",
    "        return total_loss / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T12:49:17.157563Z",
     "iopub.status.busy": "2025-05-15T12:49:17.157326Z",
     "iopub.status.idle": "2025-05-15T12:49:17.178967Z",
     "shell.execute_reply": "2025-05-15T12:49:17.178263Z",
     "shell.execute_reply.started": "2025-05-15T12:49:17.157548Z"
    },
    "id": "t1qRzp2YWZBo",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_yolo_output_to_boxes(output, grid_size=7, num_boxes=2, num_classes=20, conf_thresh=0.25, iou_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Convert YOLO model output to bounding boxes, class predictions, and confidence scores.\n",
    "    Applies Non-Maximum Suppression (NMS).\n",
    "    \"\"\"\n",
    "    batch_size = output.shape[0]\n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    all_class_idxs = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        boxes = []\n",
    "        scores = []\n",
    "        class_idxs = []\n",
    "\n",
    "        pred = output[b]  \n",
    "\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                cell_pred = pred[i, j]\n",
    "\n",
    "                for box_idx in range(num_boxes):\n",
    "                    box_start = box_idx * 5\n",
    "                    confidence = cell_pred[box_start + 4].item()\n",
    "\n",
    "                    if confidence < conf_thresh:\n",
    "                        continue\n",
    "\n",
    "                    x_cell, y_cell = cell_pred[box_start:box_start+2]\n",
    "                    w, h = cell_pred[box_start+2:box_start+4]\n",
    "\n",
    "                    x_center = (j + x_cell) / grid_size\n",
    "                    y_center = (i + y_cell) / grid_size\n",
    "                    w = w.clamp(0, 1)\n",
    "                    h = h.clamp(0, 1)\n",
    "\n",
    "                    x1 = (x_center - w/2).clamp(0, 1)\n",
    "                    y1 = (y_center - h/2).clamp(0, 1)\n",
    "                    x2 = (x_center + w/2).clamp(0, 1)\n",
    "                    y2 = (y_center + h/2).clamp(0, 1)\n",
    "\n",
    "                    box = [x1.item() * 448, y1.item() * 448, x2.item() * 448, y2.item() * 448]\n",
    "\n",
    "                    class_scores = cell_pred[num_boxes*5:]\n",
    "                    class_idx = torch.argmax(class_scores).item()\n",
    "                    class_score = class_scores[class_idx].item()\n",
    "\n",
    "                    score = class_score * confidence\n",
    "\n",
    "                    boxes.append(box)\n",
    "                    scores.append(score)\n",
    "                    class_idxs.append(class_idx)\n",
    "\n",
    "        boxes = torch.tensor(boxes)\n",
    "        scores = torch.tensor(scores)\n",
    "        class_idxs = torch.tensor(class_idxs)\n",
    "\n",
    "        # Apply NMS per class\n",
    "        keep_boxes = []\n",
    "        keep_scores = []\n",
    "        keep_classes = []\n",
    "        for cls in range(num_classes):\n",
    "            inds = (class_idxs == cls).nonzero(as_tuple=True)[0]\n",
    "            if inds.numel() == 0:\n",
    "                continue\n",
    "            cls_boxes = boxes[inds]\n",
    "            cls_scores = scores[inds]\n",
    "            keep = nms(cls_boxes, cls_scores, iou_thresh)\n",
    "            keep_boxes.append(cls_boxes[keep])\n",
    "            keep_scores.append(cls_scores[keep])\n",
    "            keep_classes.append(torch.full((len(keep),), cls, dtype=torch.int64))\n",
    "\n",
    "        if keep_boxes:\n",
    "            all_boxes.append(torch.cat(keep_boxes, dim=0))\n",
    "            all_scores.append(torch.cat(keep_scores, dim=0))\n",
    "            all_class_idxs.append(torch.cat(keep_classes, dim=0))\n",
    "        else:\n",
    "            all_boxes.append(torch.zeros((0, 4)))\n",
    "            all_scores.append(torch.zeros(0))\n",
    "            all_class_idxs.append(torch.zeros(0, dtype=torch.int64))\n",
    "\n",
    "    return all_boxes, all_scores, all_class_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T12:49:17.179918Z",
     "iopub.status.busy": "2025-05-15T12:49:17.179662Z",
     "iopub.status.idle": "2025-05-15T12:49:17.199801Z",
     "shell.execute_reply": "2025-05-15T12:49:17.199102Z",
     "shell.execute_reply.started": "2025-05-15T12:49:17.179903Z"
    },
    "id": "fhFhNZVJWdhP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "\n",
    "    x1 = torch.max(box1[0], box2[0])\n",
    "    y1 = torch.max(box1[1], box2[1])\n",
    "    x2 = torch.min(box1[2], box2[2])\n",
    "    y2 = torch.min(box1[3], box2[3])\n",
    "\n",
    "    if x2 < x1 or y2 < y1:\n",
    "        return 0.0\n",
    "\n",
    "    intersection_area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "    return iou\n",
    "\n",
    "def calculate_map(\n",
    "    all_pred_boxes_batch,  \n",
    "    all_pred_scores_batch, \n",
    "    all_pred_classes_batch,\n",
    "    all_gt_boxes_batch,   \n",
    "    all_gt_classes_batch,  \n",
    "    num_classes=20,\n",
    "    iou_threshold=0.5\n",
    "):\n",
    "    average_precisions = {}\n",
    "    epsilon = 1e-8 \n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "\n",
    "        detections_for_class = []  \n",
    "        ground_truths_for_class_by_image = defaultdict(list) \n",
    "        total_gt_for_class = 0\n",
    "\n",
    "        for i in range(len(all_pred_boxes_batch)): \n",
    "       \n",
    "            gt_boxes_img = all_gt_boxes_batch[i]\n",
    "            gt_classes_img = all_gt_classes_batch[i]\n",
    "\n",
    "            class_gt_indices = (gt_classes_img == class_idx).nonzero(as_tuple=True)[0]\n",
    "            for gt_idx in class_gt_indices:\n",
    "                ground_truths_for_class_by_image[i].append({\n",
    "                    'box': gt_boxes_img[gt_idx],\n",
    "                    'used': False\n",
    "                })\n",
    "                total_gt_for_class += 1\n",
    "\n",
    "            pred_boxes_img = all_pred_boxes_batch[i]\n",
    "            pred_scores_img = all_pred_scores_batch[i]\n",
    "            pred_classes_img = all_pred_classes_batch[i]\n",
    "\n",
    "            class_pred_indices = (pred_classes_img == class_idx).nonzero(as_tuple=True)[0]\n",
    "            for pred_idx in class_pred_indices:\n",
    "                detections_for_class.append({\n",
    "                    'score': pred_scores_img[pred_idx].item(), \n",
    "                    'image_idx': i,\n",
    "                    'box': pred_boxes_img[pred_idx]\n",
    "                })\n",
    "\n",
    "        if total_gt_for_class == 0:\n",
    "            average_precisions[class_idx] = 0.0\n",
    "            continue\n",
    "\n",
    "        if not detections_for_class:\n",
    "            average_precisions[class_idx] = 0.0\n",
    "            continue\n",
    "\n",
    "        detections_for_class.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        num_detections = len(detections_for_class)\n",
    "        tp_arr = torch.zeros(num_detections)\n",
    "        fp_arr = torch.zeros(num_detections)\n",
    "\n",
    "        for det_idx, det in enumerate(detections_for_class):\n",
    "            img_idx_of_det = det['image_idx']\n",
    "            pred_box = det['box']\n",
    "\n",
    "            gt_objects_in_image = ground_truths_for_class_by_image[img_idx_of_det]\n",
    "\n",
    "            best_iou = -1.0\n",
    "            best_gt_match_idx = -1\n",
    "\n",
    "            for gt_obj_idx, gt_obj in enumerate(gt_objects_in_image):\n",
    "                iou = calculate_iou(pred_box, gt_obj['box'])\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_match_idx = gt_obj_idx\n",
    "\n",
    "            if best_iou >= iou_threshold:\n",
    "          \n",
    "                if not gt_objects_in_image[best_gt_match_idx]['used']:\n",
    "                    tp_arr[det_idx] = 1\n",
    "                    gt_objects_in_image[best_gt_match_idx]['used'] = True\n",
    "                else: \n",
    "                    fp_arr[det_idx] = 1\n",
    "            else: \n",
    "                fp_arr[det_idx] = 1\n",
    "\n",
    "        # 4. Calculate Precision and Recall\n",
    "        tp_cumsum = torch.cumsum(tp_arr, dim=0)\n",
    "        fp_cumsum = torch.cumsum(fp_arr, dim=0)\n",
    "\n",
    "        recalls = tp_cumsum / (total_gt_for_class + epsilon)\n",
    "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + epsilon)\n",
    "\n",
    "        precisions = torch.cat((torch.tensor([1.0]), precisions)) \n",
    "        recalls = torch.cat((torch.tensor([0.0]), recalls))     \n",
    "\n",
    "        for i in range(len(precisions) - 2, -1, -1): \n",
    "            precisions[i] = torch.max(precisions[i], precisions[i+1])\n",
    "\n",
    "        recall_changes_indices = torch.where(recalls[1:] != recalls[:-1])[0]\n",
    "\n",
    "        ap = torch.sum((recalls[recall_changes_indices + 1] - recalls[recall_changes_indices]) * precisions[recall_changes_indices + 1])\n",
    "\n",
    "        average_precisions[class_idx] = ap.item()\n",
    "\n",
    "    # Calculate mAP\n",
    "    valid_aps = [ap for ap in average_precisions.values() if not torch.isnan(torch.tensor(ap))] \n",
    "    if not valid_aps:\n",
    "         mean_ap = 0.0\n",
    "    else:\n",
    "        mean_ap = sum(valid_aps) / len(valid_aps) if valid_aps else 0.0\n",
    "\n",
    "    return mean_ap, average_precisions\n",
    "\n",
    "def calculate_precision_recall(pred_boxes, pred_scores, pred_classes, gt_boxes, gt_classes, num_classes=20, iou_threshold=0.5):\n",
    "\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "\n",
    "    for i in range(len(pred_boxes)):\n",
    "     \n",
    "        boxes = pred_boxes[i]\n",
    "        scores = pred_scores[i]\n",
    "        classes = pred_classes[i]\n",
    "\n",
    "        gt_b = gt_boxes[i]\n",
    "        gt_c = gt_classes[i]\n",
    "\n",
    "        gt_detected = torch.zeros(len(gt_b))\n",
    "\n",
    "        for j in range(len(boxes)):\n",
    "        \n",
    "            max_iou = 0\n",
    "            max_idx = -1\n",
    "\n",
    "            for k in range(len(gt_b)):\n",
    "     \n",
    "                if gt_c[k] != classes[j]:\n",
    "                    continue\n",
    "\n",
    "                iou = calculate_iou(boxes[j], gt_b[k])\n",
    "                if iou > max_iou:\n",
    "                    max_iou = iou\n",
    "                    max_idx = k\n",
    "\n",
    "            if max_iou >= iou_threshold and gt_detected[max_idx] == 0:\n",
    "                total_tp += 1\n",
    "                gt_detected[max_idx] = 1 \n",
    "            else:\n",
    "                total_fp += 1\n",
    "\n",
    "        total_fn += (1 - gt_detected).sum().item()\n",
    "\n",
    "    precision = total_tp / (total_tp + total_fp + 1e-8)\n",
    "    recall = total_tp / (total_tp + total_fn + 1e-8)\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T12:49:17.200607Z",
     "iopub.status.busy": "2025-05-15T12:49:17.200422Z",
     "iopub.status.idle": "2025-05-15T12:49:17.219097Z",
     "shell.execute_reply": "2025-05-15T12:49:17.218393Z",
     "shell.execute_reply.started": "2025-05-15T12:49:17.200591Z"
    },
    "id": "ImrOM-c8WmY_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_pred_boxes = []\n",
    "    all_pred_scores = []\n",
    "    all_pred_classes = []\n",
    "    all_gt_boxes = []\n",
    "    all_gt_classes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _, gt_boxes, gt_classes, _ in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Convert outputs to boxes\n",
    "            pred_boxes, pred_scores, pred_classes = convert_yolo_output_to_boxes(outputs)\n",
    "\n",
    "            # Append to lists\n",
    "            all_pred_boxes.extend(pred_boxes)\n",
    "            all_pred_scores.extend(pred_scores)\n",
    "            all_pred_classes.extend(pred_classes)\n",
    "            all_gt_boxes.extend(gt_boxes)\n",
    "            all_gt_classes.extend(gt_classes)\n",
    "\n",
    "    # Calculate metrics\n",
    "    map_score, ap_per_class = calculate_map(all_pred_boxes, all_pred_scores, all_pred_classes,\n",
    "                                           all_gt_boxes, all_gt_classes)\n",
    "    precision, recall = calculate_precision_recall(all_pred_boxes, all_pred_scores, all_pred_classes,\n",
    "                                                 all_gt_boxes, all_gt_classes)\n",
    "\n",
    "    return map_score, precision, recall, ap_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T12:49:17.220073Z",
     "iopub.status.busy": "2025-05-15T12:49:17.219872Z",
     "iopub.status.idle": "2025-05-15T12:49:17.238983Z",
     "shell.execute_reply": "2025-05-15T12:49:17.238163Z",
     "shell.execute_reply.started": "2025-05-15T12:49:17.220049Z"
    },
    "id": "hyQPER5nWtlA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def show_image_with_boxes_nms(image, target, grid_size=7, class_names=CLASS_NAMES, conf_thresh=0.5):\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            cell = target[i, j]\n",
    "\n",
    "            # Get both boxes and their confidences\n",
    "            box1 = cell[0:5]\n",
    "            box2 = cell[5:10]\n",
    "            conf1 = box1[4].item()\n",
    "            conf2 = box2[4].item()\n",
    "\n",
    "            # Choose the box with the higher confidence\n",
    "            if conf1 > conf2:\n",
    "                box = box1\n",
    "                conf = conf1\n",
    "            else:\n",
    "                box = box2\n",
    "                conf = conf2\n",
    "\n",
    "            if conf < conf_thresh:\n",
    "                continue\n",
    "\n",
    "            # Box coordinates\n",
    "            x, y, box_w, box_h = box[:4]\n",
    "            x = ((j + x) / grid_size * w).item()\n",
    "            y = ((i + y) / grid_size * h).item()\n",
    "            bw = (box_w * w).item()\n",
    "            bh = (box_h * h).item()\n",
    "\n",
    "            # Draw bounding box\n",
    "            rect = patches.Rectangle((x - bw/2, y - bh/2), bw, bh, linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Get class prediction\n",
    "            class_probs = cell[10:]\n",
    "            if class_probs.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            class_idx = class_probs.argmax().item()\n",
    "            class_score = class_probs[class_idx].item()\n",
    "            class_name = class_names[class_idx]\n",
    "\n",
    "            ax.text(x - bw/2, y - bh/2 - 5, f'{class_name} ({class_score:.2f})', color='white',\n",
    "                    fontsize=8, backgroundcolor='red')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-15T12:49:17.239941Z",
     "iopub.status.busy": "2025-05-15T12:49:17.239678Z",
     "iopub.status.idle": "2025-05-15T12:49:21.319206Z",
     "shell.execute_reply": "2025-05-15T12:49:21.318385Z",
     "shell.execute_reply.started": "2025-05-15T12:49:17.239917Z"
    },
    "id": "6Uz5kwDaTECd",
    "outputId": "40e4ef14-260b-4c07-af7b-2f9a3cc26e85",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YOLOResNet().to(device)\n",
    "#model.load_state_dict(torch.load('/content/yolo_model_weights_2.pth', map_location=device))\n",
    "\n",
    "criterion = YOLOLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_maps = []\n",
    "val_precisions = []\n",
    "val_recalls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-15T12:49:21.320324Z",
     "iopub.status.busy": "2025-05-15T12:49:21.320081Z",
     "iopub.status.idle": "2025-05-15T15:40:37.052671Z",
     "shell.execute_reply": "2025-05-15T15:40:37.051776Z",
     "shell.execute_reply.started": "2025-05-15T12:49:21.320301Z"
    },
    "id": "wkyENAHUTECe",
    "outputId": "6c146b09-cca0-408b-fa86-808cfaaccb37",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "    for images, targets, _, _, _ in loop:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets, _, _, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # EVALUATION\n",
    "    print(\"\\nCalculating validation metrics...\")\n",
    "    map_score, precision, recall, ap_per_class = evaluate_model(model, val_loader, device)\n",
    "    val_maps.append(map_score)\n",
    "    val_precisions.append(precision)\n",
    "    val_recalls.append(recall)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Val mAP@0.5: {map_score:.4f}\")\n",
    "    print(f\"  Val Precision: {precision:.4f}\")\n",
    "    print(f\"  Val Recall: {recall:.4f}\")\n",
    "\n",
    "    if isinstance(scheduler, ReduceLROnPlateau):\n",
    "      scheduler.step(map_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T15:40:37.081628Z",
     "iopub.status.busy": "2025-05-15T15:40:37.081327Z",
     "iopub.status.idle": "2025-05-15T15:40:39.177961Z",
     "shell.execute_reply": "2025-05-15T15:40:39.177150Z",
     "shell.execute_reply.started": "2025-05-15T15:40:37.081601Z"
    },
    "id": "yVoCYWlcTECe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"yolo_model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-15T15:40:39.179218Z",
     "iopub.status.busy": "2025-05-15T15:40:39.178956Z",
     "iopub.status.idle": "2025-05-15T15:41:04.154281Z",
     "shell.execute_reply": "2025-05-15T15:41:04.153337Z",
     "shell.execute_reply.started": "2025-05-15T15:40:39.179196Z"
    },
    "id": "6h37u9dFXUer",
    "outputId": "2807e62a-7a9e-4ff1-90f8-97a9bfefbcd8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Final Evaluation on Test Set ===\")\n",
    "test_map, test_precision, test_recall, test_ap_per_class = evaluate_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Test mAP@0.5: {test_map:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "print(\"\\nClass-wise AP scores:\")\n",
    "for cls_idx, ap in test_ap_per_class.items():\n",
    "    print(f\"{CLASS_NAMES[cls_idx]}: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "execution": {
     "iopub.execute_input": "2025-05-15T15:41:04.155966Z",
     "iopub.status.busy": "2025-05-15T15:41:04.155463Z",
     "iopub.status.idle": "2025-05-15T15:41:05.119658Z",
     "shell.execute_reply": "2025-05-15T15:41:05.118839Z",
     "shell.execute_reply.started": "2025-05-15T15:41:04.155939Z"
    },
    "id": "JO3DWTAoXeuX",
    "outputId": "b309daee-5b0b-4097-e7ab-100fd036226d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize_prediction_with_metrics(model, image, target, original_boxes, original_labels, iou_threshold=0.5, score_threshold=0.3):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get model prediction\n",
    "        input_tensor = image.unsqueeze(0).to(device)\n",
    "        output = model(input_tensor).squeeze(0)\n",
    "\n",
    "        # Convert output to boxes\n",
    "        pred_boxes, pred_scores, pred_classes = convert_yolo_output_to_boxes(output.unsqueeze(0))\n",
    "        pred_boxes = pred_boxes[0]\n",
    "        pred_scores = pred_scores[0]\n",
    "        pred_classes = pred_classes[0]\n",
    "\n",
    "        # Filter by score threshold\n",
    "        keep = pred_scores > score_threshold\n",
    "        pred_boxes = pred_boxes[keep]\n",
    "        pred_scores = pred_scores[keep]\n",
    "        pred_classes = pred_classes[keep]\n",
    "\n",
    "        # Apply NMS\n",
    "        if len(pred_boxes) > 0:\n",
    "            keep_idx = nms(pred_boxes, pred_scores, iou_threshold)\n",
    "            pred_boxes = pred_boxes[keep_idx]\n",
    "            pred_scores = pred_scores[keep_idx]\n",
    "            pred_classes = pred_classes[keep_idx]\n",
    "\n",
    "        # Visualization code (same as before)\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "        h, w = image_np.shape[:2]\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "        # Ground truth\n",
    "        ax1.imshow(image_np)\n",
    "        ax1.set_title(\"Ground Truth\")\n",
    "\n",
    "        for box, label in zip(original_boxes, original_labels):\n",
    "            x1, y1, x2, y2 = box\n",
    "            class_idx = label.item()\n",
    "            class_name = CLASS_NAMES[class_idx]\n",
    "\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='g', facecolor='none')\n",
    "            ax1.add_patch(rect)\n",
    "            ax1.text(x1, y1-5, class_name, color='white', fontsize=8, backgroundcolor='green')\n",
    "\n",
    "        # Predictions\n",
    "        ax2.imshow(image_np)\n",
    "        ax2.set_title(\"Predictions\")\n",
    "\n",
    "        for box, score, class_idx in zip(pred_boxes, pred_scores, pred_classes):\n",
    "            x1, y1, x2, y2 = box\n",
    "            class_name = CLASS_NAMES[class_idx.item()]\n",
    "\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax2.add_patch(rect)\n",
    "            ax2.text(x1, y1-5, f'{class_name} ({score:.2f})', color='white', fontsize=8, backgroundcolor='red')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Visualize a sample from test set\n",
    "sample_idx = 99  # Change this to view different samples\n",
    "img, target, orig_boxes, orig_labels, _ = test_dataset[sample_idx]\n",
    "visualize_prediction_with_metrics(model, img, target, orig_boxes, orig_labels)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
